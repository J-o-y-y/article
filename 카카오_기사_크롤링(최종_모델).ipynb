{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xtOyy_RY2od-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import re\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EBwnfgQ42ytK"
      },
      "outputs": [],
      "source": [
        "def KakaoArticle(start_date, end_date): # 각 파라미터에 대한 입력값 형식은 '2023.10.26'으로 str 형식 이여야함\n",
        "  # csv 파일 쓰기\n",
        "  f = open(\"news(2022.01.02).csv\", \"w\", newline=\"\", encoding=\"utf-8\")\n",
        "  wr = csv.writer(f)\n",
        "  index = [\"기사날짜\", \"기사제목\", \"네이버 뉴스 url\",\"기사내용\"]\n",
        "  wr.writerow(index)\n",
        "\n",
        "  page = 1 # 기사 페이지\n",
        "  while True:\n",
        "    url = 'https://search.naver.com/search.naver?where=news&query=%22%EC%B9%B4%EC%B9%B4%EC%98%A4%22&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds=2022.01.02&de=2022.01.02&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom20220102to20220102&is_sug_officeid=0&office_category=0&service_area=0'\n",
        "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}\n",
        "    res = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(res.text, 'html.parser')\n",
        "    csv_value = []\n",
        "    title = soup.select('div.news_contents > a.news_tit')\n",
        "    if len(title) == 0: # 이 경우 기사가 검색되지 않고, 검색결과가 없다는 검색결과가 표시됨\n",
        "      break\n",
        "    else:\n",
        "      title_list = [] # 기자제목 리스트\n",
        "      for i in title:\n",
        "        title_list.append(i['title'])\n",
        "      naver_article = soup.select('div.info_group')\n",
        "      article_list = []\n",
        "      for i in naver_article:\n",
        "        article_list.append(i)\n",
        "      # 네이버 기사 링크 리스트로 정리\n",
        "      naver_article_url = []\n",
        "      for i in article_list:\n",
        "        try:\n",
        "          naver_article_url.append(i.select('a')[1]['href'])\n",
        "        except:\n",
        "          naver_article_url.append('url없음')\n",
        "      # 기사 날짜 설정\n",
        "      date = soup.select('div.info_group > span')\n",
        "      date_list = []\n",
        "      for i in date:\n",
        "        date_list.append(i.text)\n",
        "      # 날짜형식(xxxx.xx.xx.)이 나오게 수정\n",
        "      article_date = []\n",
        "      for i in date_list:\n",
        "        p = re.compile('[0-9][0-9][0-9][0-9].[0-9][0-9].[0-9][0-9].')\n",
        "        t = re.compile('[0-9가-힣]* 전')\n",
        "        if p.match(i) != None or t.match(i) != None:\n",
        "          article_date.append(i)\n",
        "      # 기사 내용 크롤링\n",
        "      content_list = [] # 기사 내용이 담길 리스트\n",
        "      for i in naver_article_url:\n",
        "        if i == 'url없음':\n",
        "          content_list.append('<네이버 기사 없음>')\n",
        "        else:\n",
        "          content_url = i # 해당 기사 링크로 진입\n",
        "          content_res = requests.get(content_url, headers=headers)\n",
        "          content_soup = BeautifulSoup(content_res.text, 'html.parser')\n",
        "          content = content_soup.select_one('article.go_trans') # 대부분의 기사 내용 구조가 이러함\n",
        "          if len(content_soup.select('article.go_trans')) != 0: # 해당 기사 내용이 있다면\n",
        "            content_list.append(content.text.strip().replace('\\n',''))\n",
        "          else: # 위의 구조에도 하나도 포함되지 않다면! => 해당 기사에 대한 내용 구조를 찾아야 할 수 있음!\n",
        "            content_list.append('<구조 다름>')\n",
        "      for i in range(len(title_list)):\n",
        "        temp = []\n",
        "        temp.append(article_date[i])\n",
        "        temp.append(title_list[i])\n",
        "        temp.append(naver_article_url[i])\n",
        "        temp.append(content_list[i])\n",
        "        csv_value.append(temp)\n",
        "      print('기사 페이지: ', page)\n",
        "      print('=================================================================')\n",
        "      for i in csv_value:\n",
        "        wr.writerow(i)\n",
        "    page += 1\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6saBeM26pjU",
        "outputId": "b98126d5-b290-4b43-ddc9-cf3fba05bf39"
      },
      "outputs": [],
      "source": [
        "KakaoArticle('2022.01.02','2022.01.02') # 코렙 파일시스템에서 /content/news.csv로 생성됨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uaY4x4n6vUI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
